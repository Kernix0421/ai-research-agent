import os
from dotenv import load_dotenv
load_dotenv()
import re
import time
import json
import pickle
from datetime import datetime
from models import qwen, deepseek

# --- [1. ç¯å¢ƒé…ç½®] ---
os.environ["HF_HUB_OFFLINE"] = "1"
os.environ["TRANSFORMERS_OFFLINE"] = "1"
os.environ["HF_HOME"] = "D:/AI_project/hf_cache"

from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_community.retrievers import BM25Retriever
# --- ä¿®æ­£åçš„å¯¼å…¥éƒ¨åˆ† (D:/AI_project/ask.py) ---

# å½»åº•æ”¾å¼ƒè‡ªåŠ¨æœç´¢ï¼Œæ‰‹åŠ¨å°è¯•æ‰€æœ‰å¯èƒ½çš„ç‰©ç†è·¯å¾„
import langchain
try:
    # è·¯å¾„ 1: ç°ä»£ç‰ˆæœ¬æ ‡å‡†è·¯å¾„
    from langchain.retrievers.ensemble_retriever import EnsembleRetriever
except ImportError:
    try:
        # è·¯å¾„ 2: æŸäº› 0.3.x çš„å˜ä½“è·¯å¾„
        from langchain.retrievers import EnsembleRetriever
    except ImportError:
        try:
            # è·¯å¾„ 3: å¼ºåˆ¶ä»æ¨¡å—æ ¹ç›®å½•å¯¼å…¥
            import importlib
            mod = importlib.import_module("langchain.retrievers.ensemble_retriever")
            EnsembleRetriever = mod.EnsembleRetriever
        except Exception as e:
            st.error(f"ä¸¥é‡é”™è¯¯ï¼šæ— æ³•å®šä½æ£€ç´¢ç»„ä»¶ã€‚è¯·æ£€æŸ¥å®‰è£…ã€‚é”™è¯¯ä¿¡æ¯: {e}")

# ç¡®ä¿å…¶ä»–æ£€ç´¢ç»„ä»¶ä¹Ÿæ­£å¸¸
from langchain_community.vectorstores import FAISS
from langchain_community.retrievers import BM25Retriever

# è·¯å¾„å¸¸é‡
DB_PATH = os.getenv("FAISS_INDEX_PATH", "./vectorstore")
BM25_PATH = os.getenv("BM25_PICKLE_PATH", "./bm25_data.pkl")
LOG_FILE = os.getenv("RESEARCH_LOG_PATH", "./logs/research_agents_log.json")


# --- [2. å†å²æ—¥å¿—ä¸“å®¶] ---
class HistoryLogger:
    def __init__(self, filepath):
        self.filepath = filepath
        log_dir = os.path.dirname(filepath)
        if log_dir:  # é¿å… filepath æ˜¯çº¯æ–‡ä»¶åï¼ˆå¦‚ "log.json"ï¼‰æ—¶å‡ºé”™
            os.makedirs(log_dir, exist_ok=True)

        # å¦‚æœæ—¥å¿—æ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆå§‹åŒ–ä¸ºç©ºåˆ—è¡¨
        if not os.path.exists(filepath):
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump([], f)

    def log(self, data):
        os.makedirs(os.path.dirname(LOG_FILE), exist_ok=True)
        try:
            with open(self.filepath, 'r', encoding='utf-8') as f:
                logs = json.load(f)
            logs.append({**{"timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S")}, **data})
            with open(self.filepath, 'w', encoding='utf-8') as f:
                json.dump(logs, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"âš ï¸ æ—¥å¿—å†™å…¥å¤±è´¥: {e}")


# --- [3. Agent åä½œå°ç»„å®šä¹‰] ---

class ResearchAgent:
    """èµ„æ–™å‘˜ï¼šè´Ÿè´£æ··åˆæ£€ç´¢ä¸è¯æ®æ¸…æ´—"""

    def __init__(self, retriever):
        self.retriever = retriever

    def work(self, query):
        print("ğŸ” [èµ„æ–™å‘˜] æ£€ç´¢ä¸­...")
        docs = self.retriever.invoke(query)
        context = "\n\n".join([f"ã€æ¥æº:{os.path.basename(d.metadata['source'])}ã€‘\n{d.page_content}" for d in docs])
        return context, docs


class WritingAgent:
    """å†™æ‰‹ï¼šè´Ÿè´£å†…å®¹ç”Ÿæˆä¸ Token æ§åˆ¶"""

    def __init__(self, llm, max_tokens):
        self.llm = llm
        self.max_tokens = max_tokens

    def work(self, query, context, history, task_type):
        print(f"âœï¸ [å†™æ‰‹] æ’°å†™ä¸­ (é™é¢:{self.max_tokens})...")

        mode_str = "ç§‘ç ”é—®ç­”" if task_type == "1" else "é¡¹ç›®ç”³è¯·è¾…åŠ©"
        prompt = f"""ä½ æ˜¯ä¸€å{mode_str}ä¸“å®¶ã€‚è¯·åŸºäºè¯æ®å›ç­”ï¼Œä¸¥ç¦å¹»è§‰ã€‚

ã€å†å²è®°å¿†ã€‘ï¼š{history}
ã€äº‹å®è¯æ®ã€‘ï¼š{context}
ã€å½“å‰é—®é¢˜ã€‘ï¼š{query}

è¦æ±‚ï¼šåˆ†ç‚¹å™è¿°ï¼Œæ¯æ¡ç»“è®ºååŠ (æ¥æº:æ–‡ä»¶å)ã€‚"""

        # ç»‘å®š Token é™åˆ¶
        limited_llm = self.llm.bind(max_tokens=self.max_tokens)
        response = limited_llm.invoke(prompt)

        content = response.content
        finish_reason = response.response_metadata.get("finish_reason", "")
        return content, (finish_reason == "length")


class EvaluatorAgent:
    """å‡çº§ç‰ˆè¯„ä¼°å‘˜ï¼šé‡åŒ–ç§‘ç ”æŒ‡æ ‡"""

    def __init__(self, llm):
        self.llm = llm

    def work(self, query, draft, context):
        prompt = f"""ä½ æ˜¯ä¸€åä¸¥è°¨çš„ç§‘ç ”è¯„å®¡ã€‚è¯·å¯¹æ¯”ã€äº‹å®è¯æ®ã€‘ä¸ã€æ¨¡å‹å›ç­”ã€‘ï¼Œè¿›è¡Œé‡åŒ–æ‰“åˆ†ã€‚

ã€äº‹å®è¯æ®ã€‘ï¼š{context}
ã€æ¨¡å‹å›ç­”ã€‘ï¼š{draft}

è¯·ä¸¥æ ¼æŒ‰ä»¥ä¸‹ JSON æ ¼å¼è¾“å‡ºç»“æœï¼ˆä¸è¦åŒ…å«å…¶ä»–æ–‡å­—ï¼‰ï¼š
{{
  "accuracy": 0-100,      // å›ç­”ä¸è¯æ®çš„å¥‘åˆåº¦
  "precision": 0-100,     // å›ç­”ä¸­æœ‰æ•ˆä¿¡æ¯å æ¯”
  "recall": 0-100,        // è¯æ®ä¸­å…³é”®ç‚¹è¢«é‡‡çº³çš„æ¯”ä¾‹
  "hallucination": 0-100, // å¹»è§‰ç‡ï¼ˆè¯æ®ä¸­æœªæåŠå†…å®¹çš„æ¯”ä¾‹ï¼‰
  "reason": "ç®€çŸ­çš„é‡åŒ–åˆ†æç†ç”±"
}}
"""
        # å¼ºåˆ¶ä½¿ç”¨ JSON æ¨¡å¼
        response = self.llm.bind(max_tokens=500).invoke(prompt)
        try:
            # æå–å¹¶è§£æ JSON
            import json
            res = json.loads(re.search(r'\{.*\}', response.content, re.S).group())
            return res
        except:
            # ä¿åº•æ–¹æ¡ˆ
            return {"accuracy": 0, "precision": 0, "recall": 0, "hallucination": 0, "reason": "è§£æå¤±è´¥"}


# --- [4. è°ƒåº¦ä¸­å¿ƒ (Orchestrator)] ---

class ResearchOrchestrator:
    def __init__(self):
        self.logger = HistoryLogger(LOG_FILE)
        self.retriever = self._init_retriever()
        self.researcher = ResearchAgent(self.retriever)
        self.chat_memory = ""  # ç®€å•è®°å¿†

    def _init_retriever(self):
        print("âŒ› [åˆå§‹åŒ–] åŠ è½½ Embedding ä¸ç´¢å¼•...")
        emb = HuggingFaceEmbeddings(model_name="BAAI/bge-m3", model_kwargs={'device': 'cpu'})
        vs = FAISS.load_local(DB_PATH, emb, allow_dangerous_deserialization=True)
        with open(BM25_PATH, "rb") as f: bm25_data = pickle.load(f)
        bm25 = BM25Retriever.from_documents(bm25_data)
        bm25.k = 4
        return EnsembleRetriever(retrievers=[vs.as_retriever(search_kwargs={"k": 7}), bm25], weights=[0.7, 0.3])

    def execute(self, query, task_type, model_choice, t_limit):
        # æ¨¡å‹åˆ†é…ï¼šå†™æ‰‹(DeepSeeké€»è¾‘å¼º), è¯„ä¼°å‘˜(QwenæŒ‡ä»¤å¼º)
        writer_llm = deepseek if model_choice == "3" else qwen
        eval_llm = qwen

        writer = WritingAgent(writer_llm, max_tokens=t_limit)
        evaluator = EvaluatorAgent(eval_llm)

        start_time = time.time()

        # 1. æ£€ç´¢
        context, raw_docs = self.researcher.work(query)

        # 2. å†™ä½œ
        draft, is_truncated = writer.work(query, context, self.chat_memory, task_type)
        if is_truncated: draft = "âš ï¸(å†…å®¹å› é•¿åº¦æˆªæ–­)\n" + draft

        # 3. è¯„ä¼°
        score, feedback = evaluator.work(query, draft, context)

        # 4. å­˜å…¥è®°å¿†ä¸æ—¥å¿—
        duration = f"{time.time() - start_time:.2f}s"
        self.chat_memory = f"Q:{query} A:{draft[:100]}..."  # è®°å¿†å‹ç¼©

        log_data = {
            "query": query, "score": score, "duration": duration,
            "model": "DeepSeek-V3" if model_choice == "3" else "Qwen-Plus",
            "is_truncated": is_truncated
        }
        self.logger.log(log_data)

        return draft, feedback, score, duration


# --- [5. äº¤äº’ç•Œé¢] ---

def main():
    print("\n" + "=" * 60)
    print("ğŸ”¬ AIç§‘ç ”åŠ©æ‰‹ Agent V1.5 | è‡ªåŠ¨å­˜æ¡£ | Tokenä¿æŠ¤ | å¤šAgentäº’å®¡")
    print("=" * 60)

    boss = ResearchOrchestrator()

    while True:
        query = input("\n[æé—®] (qé€€å‡º): ")
        if query.lower() == 'q': break

        print("ä»»åŠ¡: 1.ç§‘ç ”é—®ç­” 2.é¡¹ç›®ç”³è¯· | æ¨¡å‹: 2.Qwen 3.DeepSeek")
        t_type = input("ä»»åŠ¡(é»˜è®¤1): ") or "1"
        m_type = input("æ¨¡å‹(é»˜è®¤3): ") or "3"
        t_limit = int(input("Tokenä¸Šé™(é»˜è®¤1000): ") or "1000")

        draft, feedback, score, cost = boss.execute(query, t_type, m_type, t_limit)

        print("\n" + "â€”" * 20 + " âœï¸ å†™æ‰‹æŠ¥å‘Š " + "â€”" * 20)
        print(draft)
        print("\n" + "â€”" * 20 + " âš–ï¸ è¯„ä¼°æ„è§ (å¾—åˆ†:{}) ".format(score) + "â€”" * 20)
        print(feedback)
        print(f"\nğŸ“Š ç»Ÿè®¡: è€—æ—¶{cost} | æ—¥å¿—å·²åŒæ­¥è‡³ JSON")


if __name__ == "__main__":
    main()