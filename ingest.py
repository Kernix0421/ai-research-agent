import os
# å¼ºåˆ¶ä½¿ç”¨å›½å†…é•œåƒç«™ï¼Œæé€Ÿå¹¶é˜²æ­¢ä¸‹è½½ä¸­æ–­
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
# è§£å†³å¯èƒ½çš„åº“å†²çª
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"
import pickle
from datetime import datetime
from pylatexenc.latex2text import LatexNodes2Text
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document
from unstructured.partition.auto import partition

# --- æ ¸å¿ƒé…ç½® ---
DATA_PATH = os.getenv("LITERATURE_DATA_PATH", "./data/literature")
DB_PATH = os.getenv("FAISS_INDEX_PATH", "./vectorstore")
BM25_PATH = os.getenv("BM25_PICKLE_PATH", "./bm25_data.pkl")
EMBED_MODEL = os.getenv("EMBEDDING_MODEL", "BAAI/bge-m3")

# åˆå§‹åŒ–ç»„ä»¶
print(f"âŒ› æ­£åœ¨åŠ è½½åµŒå…¥æ¨¡å‹ {EMBED_MODEL}...")
embeddings = HuggingFaceEmbeddings(model_name=EMBED_MODEL)
latex_converter = LatexNodes2Text()

# ç§‘ç ”çº§åˆ‡åˆ†å™¨ï¼š512 token å¤§å°ï¼Œ100 token é‡å 
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=512,
    chunk_overlap=100,
    separators=["\n\n", "\n", "ã€‚", "ï¼›", " ", ""]
)


def run_ingest():
    if not os.path.exists(DATA_PATH):
        print("âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ–‡çŒ®è·¯å¾„")
        return

    os.makedirs(os.path.dirname(DB_PATH), exist_ok=True)
    os.makedirs(os.path.dirname(BM25_PATH), exist_ok=True)

    all_chunks = []
    files = [f for f in os.listdir(DATA_PATH) if f.endswith((".pdf", ".docx"))]

    for file in files:
        file_path = os.path.join(DATA_PATH, file)
        print(f"ğŸ“„ è§£æä¸­ (åŒæ /Fastæ¨¡å¼): {file}")

        try:
            # strategy="fast" ç»•è¿‡ Tesseractï¼Œåˆ©ç”¨ PDF åæ ‡å¤„ç†åŒæ 
            elements = partition(
                filename=file_path,
                strategy="fast",
                multipage_sections=True,
                chunking_strategy="by_title",  # è¯­ä¹‰æ„ŸçŸ¥åˆ‡åˆ†
                languages=["chi_sim", "eng"]
            )

            for el in elements:
                content = el.text
                # LaTeX æ¸…æ´—
                if "$" in content or "\\" in content:
                    try:
                        content = latex_converter.latex_to_text(content)
                    except:
                        pass

                # äºŒæ¬¡ç‰©ç†åˆ‡åˆ†
                splits = text_splitter.split_text(content)
                for s in splits:
                    if len(s.strip()) < 15: continue
                    all_chunks.append(Document(
                        page_content=s,
                        metadata={
                            "source": file,
                            "type": el.category,
                            "date": datetime.now().strftime("%Y-%m-%d"),
                            "project": "è½¨é“æŠ€æœ¯ç ”ç©¶"
                        }
                    ))
        except Exception as e:
            print(f"âš ï¸ è·³è¿‡æ–‡ä»¶ {file}: {e}")

    # ä¿å­˜åŒç´¢å¼•ï¼šFAISS(è¯­ä¹‰) + BM25(å…³é”®è¯)
    if all_chunks:
        print(f"ğŸ“¦ æ„å»ºç´¢å¼•ä¸­ (å…± {len(all_chunks)} ä¸ªåˆ‡ç‰‡)...")
        vectorstore = FAISS.from_documents(all_chunks, embeddings)
        vectorstore.save_local(DB_PATH)
        with open(BM25_PATH, "wb") as f:
            pickle.dump(all_chunks, f)
        print("âœ… æ•°æ®åº“æ„å»ºå®Œæˆï¼")


if __name__ == "__main__":
    run_ingest()