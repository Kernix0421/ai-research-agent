import streamlit as st
import time
import os
from dotenv import load_dotenv
load_dotenv()
import json
import re
import pandas as pd
from datetime import datetime
from models import qwen, deepseek, qwen_local
from ask import ResearchOrchestrator, WritingAgent, EvaluatorAgent


# --- 1. åŸºç¡€é…ç½®ä¸è·¯å¾„ ---
st.set_page_config(page_title="è½¨é“äº¤é€šç§‘ç ”åŠ©æ‰‹ V2.2", page_icon="ğŸ”¬", layout="wide")
CHAT_HISTORY_DB = os.getenv("CHAT_HISTORY_PATH", "./logs/full_chat_history.json")
LOG_PATH = os.getenv("RESEARCH_LOG_PATH", "./logs/research_agents_log.json")

# --- 2. æŒä¹…åŒ–é€»è¾‘å‡½æ•° ---
def save_history_to_disk(messages):
    os.makedirs(os.path.dirname(LOG_FILE), exist_ok=True)

    with open(CHAT_HISTORY_DB, "w", encoding="utf-8") as f:
        json.dump(messages, f, ensure_ascii=False, indent=2)


def load_history_from_disk():
    if os.path.exists(CHAT_HISTORY_DB):
        try:
            with open(CHAT_HISTORY_DB, "r", encoding="utf-8") as f:
                return json.load(f)
        except:
            return []
    return []


# --- 3. å·¥å…·å‡½æ•° ---
def WritingAgent_work_proxy(orchestrator, query, context, t_type, m_idx, limit):
    # æ¨¡å‹è·¯ç”±æ˜ å°„
    model_map = {"3": deepseek, "2": qwen, "4": qwen_local}
    llm = model_map.get(m_idx, qwen)
    writer = WritingAgent(llm, max_tokens=limit)
    return writer.work(query, context, orchestrator.chat_memory, t_type)


def format_all_history(messages):
    full_md = "# å…¨é‡ç§‘ç ”å¯¹è¯å†å²\n\n"
    for m in messages:
        role = "ğŸ‘¨â€ğŸ”¬ ç ”ç©¶å‘˜" if m["role"] == "user" else "ğŸ¤– AI åŠ©æ‰‹"
        full_md += f"### {role}\n{m['content']}\n\n---\n"
    return full_md


# --- 4. åˆå§‹åŒ–åç«¯ ---
@st.cache_resource
def init_system():
    return ResearchOrchestrator()


boss = init_system()

# åˆå§‹åŒ– Session State å¹¶åŠ è½½å†å²è®°å½•
if "messages" not in st.session_state:
    st.session_state.messages = load_history_from_disk()

# --- 5. ä¾§è¾¹æ æ§åˆ¶é¢æ¿ ---
with st.sidebar:
    st.title("âš™ï¸ ç³»ç»Ÿæ§åˆ¶å°")
    task_type = st.radio("ä»»åŠ¡æ¨¡å¼", ("1. æ·±åº¦ç§‘ç ”é—®ç­”", "2. é¡¹ç›®ç”³è¯·è¾…åŠ©"))
    model_choice = st.selectbox(
        "é€‰æ‹©å†™æ‰‹å¼•æ“",
        ("3. DeepSeek-V3 (äº‘ç«¯)", "2. Qwen-Plus (äº‘ç«¯)", "4. Qwen3-VL-8B (æœ¬åœ°)"),
        index=0
    )
    token_limit = st.slider("ç”Ÿæˆ Token ä¸Šé™", 200, 4000, 1500)

    st.markdown("---")
    st.subheader("ğŸ’¾ æ•°æ®ç®¡ç†")

    # å…¨é‡å¯¼å‡º
    if st.session_state.messages:
        st.download_button(
            "ğŸ“ å¯¼å‡ºå…¨é‡å¯¹è¯å†å² (.md)",
            data=format_all_history(st.session_state.messages),
            file_name=f"Full_History_{datetime.now().strftime('%m%d_%H%M')}.md"
        )

    if st.button("ğŸ—‘ï¸ æ¸…ç©ºæ‰€æœ‰å†å²", help="å°†åˆ é™¤æœ¬åœ°å­˜æ¡£æ–‡ä»¶"):
        st.session_state.messages = []
        if os.path.exists(CHAT_HISTORY_DB): os.remove(CHAT_HISTORY_DB)
        boss.chat_memory = ""
        st.rerun()

    st.markdown("---")
    st.subheader("ğŸ“ˆ æœ€è¿‘ä»»åŠ¡è¯„ä¼°")
    if os.path.exists(LOG_PATH):
        try:
            logs = json.load(open(LOG_PATH, "r", encoding="utf-8"))
            if logs:
                df = pd.DataFrame(logs).tail(5)[["timestamp", "query", "score"]]
                st.dataframe(df, hide_index=True)
        except:
            st.caption("æ—¥å¿—æš‚ä¸å¯ç”¨")

# --- 6. ä¸»äº¤äº’ç•Œé¢ ---
st.title("ğŸ”¬ è½¨é“äº¤é€šç§‘ç ”å¢å¼ºç³»ç»Ÿ (Agentic V2.2)")
st.info("ç³»ç»Ÿå·²å°±ç»ªã€‚æ‰€æœ‰å¯¹è¯å°†å®æ—¶ä¿å­˜ï¼Œæ‚¨å¯ä»¥éšæ—¶å…³é—­å¹¶é‡æ–°æ‰“å¼€ã€‚")

# æ¸²æŸ“å†å²æ°”æ³¡
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

# è¾“å…¥æ¡†
if query := st.chat_input("è¾“å…¥ç§‘ç ”é—®é¢˜..."):
    # è®°å½•ç”¨æˆ·æ¶ˆæ¯
    st.session_state.messages.append({"role": "user", "content": query})
    with st.chat_message("user"):
        st.markdown(query)

    with st.chat_message("assistant"):
        start_time = time.time()
        status = st.status("ğŸš€ Agent å›¢é˜Ÿåä½œä¸­...", expanded=True)

        # A. æ£€ç´¢
        status.write("ğŸ” èµ„æ–™å‘˜ï¼šæ­£åœ¨è¿›è¡Œå‘é‡ä¸ BM25 æ··åˆæ£€ç´¢...")
        context, _ = boss.researcher.work(query)

        # B. å†™ä½œ
        m_idx = model_choice[0]
        status.write(f"âœï¸ å†™æ‰‹ï¼šè°ƒç”¨ {model_choice[3:]} æ’°å†™å­¦æœ¯åˆç¨¿...")
        draft, is_truncated = WritingAgent_work_proxy(boss, query, context, task_type[0], m_idx, token_limit)

        # C. è¯„ä¼° (é‡åŒ–æŒ‡æ ‡)
        status.write("âš–ï¸ è¯„ä¼°å‘˜ï¼šæ­£åœ¨è¿›è¡Œå¤šç»´æŒ‡æ ‡åˆ†æ...")
        evaluator = EvaluatorAgent(qwen)  # å›ºå®šç”¨äº‘ç«¯ Qwen ä¿è¯è£åˆ¤æƒé‡
        eval_res = evaluator.work(query, draft, context)
        # æ³¨æ„ï¼šæ­¤å¤„ eval_res éœ€ç¬¦åˆä¸Šä¸ªå›å¤ä¸­å®šä¹‰çš„ JSON æ ¼å¼

        latency = time.time() - start_time
        status.update(label=f"âœ… ä»»åŠ¡å®Œæˆ | æ€»è€—æ—¶: {latency:.1f}s", state="complete")

        # --- 7. æŒ‡æ ‡å±•ç¤ºçœ‹æ¿ ---
        st.markdown("#### ğŸ“Š æœ¬è½®ç§‘ç ”è´¨é‡æŒ‡æ ‡")
        c1, c2, c3, c4, c5 = st.columns(5)
        # å‡è®¾ eval_res åŒ…å«ï¼šaccuracy, precision, recall, hallucination, reason
        c1.metric("å‡†ç¡®ç‡", f"{eval_res.get('accuracy', 0)}%")
        c2.metric("ç²¾ç¡®åº¦", f"{eval_res.get('precision', 0)}%")
        c3.metric("å¬å›ç‡", f"{eval_res.get('recall', 0)}%")
        c4.metric("å¹»è§‰ç‡", f"{eval_res.get('hallucination', 0)}%", delta_color="inverse")
        c5.metric("è€—æ—¶", f"{latency:.1f}s")

        with st.expander("ğŸ“ æŸ¥çœ‹è¯„ä¼°ç†ç”±"):
            st.write(eval_res.get('reason', 'æš‚æ— è¯¦ç»†åˆ†æ'))

        # --- 8. ç”ŸæˆæŠ¥å‘Šä¸å¯¼å‡º ---
        st.markdown("---")
        st.markdown(draft)

        report_md = f"# ç§‘ç ”æŠ¥å‘Š: {query}\n\n**è¯„ä¼°åˆ†: {eval_res.get('accuracy')}**\n\n{draft}\n\n---\n**è¯„ä¼°æ„è§:** {eval_res.get('reason')}"
        st.download_button(
            "ğŸ“¥ å¯¼å‡ºæœ¬æ¡æŠ¥å‘Š (.md)",
            data=report_md,
            file_name=f"Report_{datetime.now().strftime('%m%d_%H%M')}.md"
        )

        if is_truncated: st.warning("âš ï¸ å†…å®¹å·²è¾¾åˆ°è®¾å®šçš„ Token ä¸Šé™ï¼Œéƒ¨åˆ†å†…å®¹è¢«æˆªæ–­ã€‚")

    # è®°å½• AI æ¶ˆæ¯å¹¶æŒä¹…åŒ–
    st.session_state.messages.append({"role": "assistant", "content": draft})
    save_history_to_disk(st.session_state.messages)